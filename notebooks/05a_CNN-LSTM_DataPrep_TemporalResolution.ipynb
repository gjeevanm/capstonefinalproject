{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca27403",
   "metadata": {},
   "source": [
    "# 05a: Data Preparation for CNN-LSTM (Temporal Resolution Experiments)\n",
    "\n",
    "This notebook prepares ICU time series and static data for CNN-LSTM modeling, enabling systematic experiments with different temporal resolutions (6, 12, 24, and 36 hours) to identify the earliest point at which accurate mortality predictions can be made.\n",
    "\n",
    "**Goals:**\n",
    "- Prepare dynamic (time series), static, and outcome data for CNN-LSTM modeling\n",
    "- Compare temporal resolutions: 6, 12, 24, and 36 hours\n",
    "- Save processed datasets for each resolution for downstream modeling and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8800da",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Data Loading and Audit\n",
    "2. Feature Selection\n",
    "3. Sequence Construction for Multiple Temporal Resolutions\n",
    "4. Padding, Scaling, and Splitting\n",
    "5. Save Processed Data for Each Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822475a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ec47e",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Audit\n",
    "\n",
    "Load the cleaned ICU dataset and missingness mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3604c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (295354, 48)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>...</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>ICUType</th>\n",
       "      <th>In-hospital_death</th>\n",
       "      <th>Length_of_stay</th>\n",
       "      <th>SAPS-I</th>\n",
       "      <th>SOFA</th>\n",
       "      <th>Survival</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>78.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132539</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>78.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132539</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>78.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132539</td>\n",
       "      <td>157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>78.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132539</td>\n",
       "      <td>188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>78.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID  Minutes  ALP  ALT  AST  Albumin   BUN  Bilirubin  Cholesterol  \\\n",
       "0    132539        7  0.0  0.0  0.0      0.0  13.0        0.0          0.0   \n",
       "1    132539       37  0.0  0.0  0.0      0.0  13.0        0.0          0.0   \n",
       "2    132539       97  0.0  0.0  0.0      0.0  13.0        0.0          0.0   \n",
       "3    132539      157  0.0  0.0  0.0      0.0  13.0        0.0          0.0   \n",
       "4    132539      188  0.0  0.0  0.0      0.0  13.0        0.0          0.0   \n",
       "\n",
       "   Creatinine  ...   Age  Gender  Height  ICUType  In-hospital_death  \\\n",
       "0         0.8  ...  54.0     0.0   170.2      4.0                0.0   \n",
       "1         0.8  ...  54.0     0.0   170.2      4.0                0.0   \n",
       "2         0.8  ...  54.0     0.0   170.2      4.0                0.0   \n",
       "3         0.8  ...  54.0     0.0   170.2      4.0                0.0   \n",
       "4         0.8  ...  54.0     0.0   170.2      4.0                0.0   \n",
       "\n",
       "   Length_of_stay  SAPS-I  SOFA  Survival  Weight  \n",
       "0             5.0     6.0   1.0      70.0    78.6  \n",
       "1             5.0     6.0   1.0      70.0    78.6  \n",
       "2             5.0     6.0   1.0      70.0    78.6  \n",
       "3             5.0     6.0   1.0      70.0    78.6  \n",
       "4             5.0     6.0   1.0      70.0    78.6  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/processed/timeseries_cleaned_all_features.csv')\n",
    "mask = pd.read_csv('../data/processed/timeseries_missingness_mask.csv')\n",
    "print('Data shape:', data.shape)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d3695",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "Select features based on EDA findings and clinical relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc76dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_features = [\n",
    "    'HR', 'SysABP', 'DiasABP', 'MAP', 'NISysABP', 'NIDiasABP', 'NIMAP', 'MechVent',\n",
    "    'RespRate', 'SaO2', 'FiO2', 'PaO2', 'PaCO2',\n",
    "    'Creatinine', 'BUN', 'Urine',\n",
    "    'Na', 'K', 'Glucose', 'Lactate', 'HCO3', 'pH',\n",
    "    'GCS', 'Temp',\n",
    "    'Shock Index', 'PaO2/FiO2 ratio', 'Pulse Pressure', 'MAP to HR ratio'\n",
    "]\n",
    "static_features = ['Age', 'Gender', 'Height', 'Weight', 'ICUType', 'SAPS-I', 'SOFA']\n",
    "target_col = 'In-hospital_death'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537500af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Feature Engineering: Add required features for sequence construction\n",
    "data['Shock Index'] = data['HR'] / data['SysABP']\n",
    "data['PaO2/FiO2 ratio'] = data['PaO2'] / data['FiO2'].replace(0, np.nan)\n",
    "data['Pulse Pressure'] = data['SysABP'] - data['DiasABP']\n",
    "data['MAP to HR ratio'] = data['MAP'] / data['HR'].replace(0, np.nan)\n",
    "# Replace inf/nan with 0 for derived features\n",
    "for col in ['Shock Index', 'PaO2/FiO2 ratio', 'Pulse Pressure', 'MAP to HR ratio']:\n",
    "    data[col] = data[col].replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2844c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sequences(data, time_series_features, hours):\n",
    "    minute_window = hours * 60\n",
    "    max_time = 2880  # 48 hours in minutes\n",
    "    window_boundaries = np.arange(0, max_time + minute_window, minute_window)\n",
    "    sequences = []\n",
    "    for record_id, group in data.groupby('RecordID'):\n",
    "        patient_sequence = []\n",
    "        for i in range(len(window_boundaries) - 1):\n",
    "            start_time = window_boundaries[i]\n",
    "            end_time = window_boundaries[i+1]\n",
    "            window_data = group[(group['Minutes'] >= start_time) & (group['Minutes'] < end_time)]\n",
    "            if not window_data.empty:\n",
    "                features = window_data[time_series_features].iloc[0].values\n",
    "            else:\n",
    "                features = np.full(len(time_series_features), np.nan)\n",
    "            patient_sequence.append(features)\n",
    "        sequences.append(patient_sequence)\n",
    "    return np.array(sequences, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002cb626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing sequences for 6-hour resolution...\n",
      "Shape for 6-hour: (3997, 8, 28)\n",
      "Constructing sequences for 12-hour resolution...\n",
      "Shape for 12-hour: (3997, 4, 28)\n",
      "Constructing sequences for 24-hour resolution...\n",
      "Shape for 24-hour: (3997, 2, 28)\n",
      "Constructing sequences for 36-hour resolution...\n",
      "Shape for 36-hour: (3997, 2, 28)\n"
     ]
    }
   ],
   "source": [
    "temporal_resolutions = [6, 12, 24, 36]\n",
    "sequences_dict = {}\n",
    "for hours in temporal_resolutions:\n",
    "    print(f'Constructing sequences for {hours}-hour resolution...')\n",
    "    X_seq = construct_sequences(data, time_series_features, hours)\n",
    "    sequences_dict[hours] = X_seq\n",
    "    print(f'Shape for {hours}-hour: {X_seq.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eadd32",
   "metadata": {},
   "source": [
    "## 4. Padding, Scaling, and Splitting\n",
    "\n",
    "For each temporal resolution, pad sequences, scale features, split data, and save processed arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548a681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed arrays for 6-hour resolution saved in results dict.\n",
      "Processed arrays for 12-hour resolution saved in results dict.\n",
      "Processed arrays for 24-hour resolution saved in results dict.\n",
      "Processed arrays for 36-hour resolution saved in results dict.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "results = defaultdict(dict)\n",
    "patient_static_data = data.groupby('RecordID')[static_features].first().reset_index()\n",
    "patient_outcomes = data.groupby('RecordID')[target_col].first().reset_index()\n",
    "X_static = patient_static_data[static_features].values\n",
    "y = patient_outcomes[target_col].values\n",
    "for hours, X_seq in sequences_dict.items():\n",
    "    max_seq_len = X_seq.shape[1]\n",
    "    X_seq_padded = pad_sequences(X_seq, maxlen=max_seq_len, dtype='float32', padding='post', value=0.0)\n",
    "    n_patients = X_seq_padded.shape[0]\n",
    "    indices = np.arange(n_patients)\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=42)\n",
    "    train_idx, val_idx  = train_test_split(train_idx, test_size=0.2, stratify=y[train_idx], random_state=42)\n",
    "    X_train, X_val, X_test = X_seq_padded[train_idx], X_seq_padded[val_idx], X_seq_padded[test_idx]\n",
    "    static_train, static_val, static_test = X_static[train_idx], X_static[val_idx], X_static[test_idx]\n",
    "    y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
    "    # Clean NaNs/Infs\n",
    "    def clean_array(arr): return np.nan_to_num(arr, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    X_train = clean_array(X_train)\n",
    "    X_val   = clean_array(X_val)\n",
    "    X_test  = clean_array(X_test)\n",
    "    static_train = clean_array(static_train)\n",
    "    static_val   = clean_array(static_val)\n",
    "    static_test  = clean_array(static_test)\n",
    "    # Scale time series features\n",
    "    n_features = X_train.shape[2]\n",
    "    seq_scaler = StandardScaler()\n",
    "    X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "    X_train_scaled = seq_scaler.fit_transform(X_train_reshaped).reshape(-1, max_seq_len, n_features)\n",
    "    X_val_reshaped = X_val.reshape(-1, n_features)\n",
    "    X_val_scaled = seq_scaler.transform(X_val_reshaped).reshape(-1, max_seq_len, n_features)\n",
    "    X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "    X_test_scaled = seq_scaler.transform(X_test_reshaped).reshape(-1, max_seq_len, n_features)\n",
    "    # Scale static features\n",
    "    static_scaler = StandardScaler()\n",
    "    static_train_scaled = static_scaler.fit_transform(static_train)\n",
    "    static_val_scaled   = static_scaler.transform(static_val)\n",
    "    static_test_scaled  = static_scaler.transform(static_test)\n",
    "    # Impute dynamic features\n",
    "    X_train_flat = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed_flat = imputer.fit_transform(X_train_flat)\n",
    "    X_val_flat  = X_val_scaled.reshape(X_val_scaled.shape[0], -1)\n",
    "    X_test_flat = X_test_scaled.reshape(X_test_scaled.shape[0], -1)\n",
    "    X_val_imputed_flat  = imputer.transform(X_val_flat)\n",
    "    X_test_imputed_flat = imputer.transform(X_test_flat)\n",
    "    X_train_imputed = X_train_imputed_flat.reshape(-1, X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    X_val_imputed   = X_val_imputed_flat.reshape(-1, X_val_scaled.shape[1], X_val_scaled.shape[2])\n",
    "    X_test_imputed  = X_test_imputed_flat.reshape(-1, X_test_scaled.shape[1], X_test_scaled.shape[2])\n",
    "    # SMOTE on training set\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_res_flat, y_train_res = smote.fit_resample(X_train_imputed_flat, y_train)\n",
    "    X_train_res = X_train_res_flat.reshape(-1, X_train_imputed.shape[1], X_train_imputed.shape[2])\n",
    "    n_orig = static_train_scaled.shape[0]\n",
    "    n_total = X_train_res.shape[0]\n",
    "    n_synth = n_total - n_orig\n",
    "    static_train_res = static_train_scaled.copy()\n",
    "    if n_synth > 0:\n",
    "        minority_class = 1 if np.sum(y_train == 1) < np.sum(y_train == 0) else 0\n",
    "        minority_indices = np.where(y_train == minority_class)[0]\n",
    "        synth_static = static_train_scaled[np.random.choice(minority_indices, size=n_synth, replace=True)]\n",
    "        static_train_res = np.concatenate([static_train_scaled, synth_static], axis=0)\n",
    "    results[hours]['X_train_final'] = X_train_res\n",
    "    results[hours]['y_train_final'] = y_train_res\n",
    "    results[hours]['static_train_final'] = static_train_res\n",
    "    results[hours]['X_val_final'] = X_val_imputed\n",
    "    results[hours]['static_val_final'] = static_val_scaled\n",
    "    results[hours]['y_val_final'] = y_val\n",
    "    results[hours]['X_test_final'] = X_test_imputed\n",
    "    results[hours]['static_test_final'] = static_test_scaled\n",
    "    results[hours]['y_test_final'] = y_test\n",
    "    print(f'Processed arrays for {hours}-hour resolution saved in results dict.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad27291",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data for Each Resolution\n",
    "\n",
    "Save the processed arrays for each temporal resolution to separate files for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2bd2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 6-hour sequence data saved to ../data/processed/cnn_lstm_6hr_data.npz\n",
      "Prepared 12-hour sequence data saved to ../data/processed/cnn_lstm_12hr_data.npz\n",
      "Prepared 24-hour sequence data saved to ../data/processed/cnn_lstm_24hr_data.npz\n",
      "Prepared 36-hour sequence data saved to ../data/processed/cnn_lstm_36hr_data.npz\n"
     ]
    }
   ],
   "source": [
    "for hours in temporal_resolutions:\n",
    "    np.savez(f'../data/processed/cnn_lstm_{hours}hr_data.npz',\n",
    "             X_train=results[hours]['X_train_final'], y_train=results[hours]['y_train_final'],\n",
    "             X_val=results[hours]['X_val_final'],   y_val=results[hours]['y_val_final'],\n",
    "             X_test=results[hours]['X_test_final'], y_test=results[hours]['y_test_final'],\n",
    "             static_train=results[hours]['static_train_final'],\n",
    "             static_val=results[hours]['static_val_final'],\n",
    "             static_test=results[hours]['static_test_final'])\n",
    "    print(f'Prepared {hours}-hour sequence data saved to ../data/processed/cnn_lstm_{hours}hr_data.npz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
